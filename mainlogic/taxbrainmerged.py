import os
import sys

"""
PII Handler, Legal Disclaimer, and Privacy Notice
Add this code to your taxbrainmerged.py file (at the top, after imports)
OR save as separate file and import
"""

import re
from typing import Dict, List, Tuple

# ==========================================
# Legal Disclaimer
# ==========================================
LEGAL_DISCLAIMER = """
âš ï¸ **IMPORTANT DISCLAIMER**

This AI Tax Assistant is for **educational and informational purposes only**.

â€¢ I am NOT a Certified Public Accountant (CPA), tax attorney, or licensed tax professional.
â€¢ This tool does NOT constitute professional tax advice, legal advice, or financial advice.
â€¢ Tax laws are complex and vary by individual circumstances. Always consult a qualified tax professional.
â€¢ You are solely responsible for the accuracy of your tax filings.
â€¢ Anthropic/OpenAI and the developers of this tool are not liable for any errors or omissions.

By using this tool, you acknowledge and accept these terms.
"""

PRIVACY_NOTICE = """
ðŸ”’ **PRIVACY & DATA HANDLING NOTICE**

â€¢ We automatically detect and mask sensitive information (SSN, EIN, account numbers).
â€¢ Your data is processed in-session only and is NOT permanently stored.
â€¢ Uploaded documents are processed temporarily and cleared after use.
â€¢ We recommend NOT uploading documents containing actual SSNs or bank account numbers.
â€¢ For maximum security, use sample/dummy data when learning how to file.

Your privacy is important to us.
"""

# ==========================================
# PII Detection and Masking Utilities
# ==========================================
class PIIHandler:
    """Handles detection and masking of Personally Identifiable Information"""
    
    # Regex patterns for sensitive data
    PATTERNS = {
        'ssn': [
            r'\b\d{3}-\d{2}-\d{4}\b',           # XXX-XX-XXXX
            r'\b\d{3}\s\d{2}\s\d{4}\b',         # XXX XX XXXX
            r'\b\d{9}\b(?!\d)',                  # XXXXXXXXX (9 digits, not followed by more digits)
        ],
        'ein': [
            r'\b\d{2}-\d{7}\b',                  # XX-XXXXXXX (EIN format)
        ],
        'bank_account': [
            r'\b\d{10,17}\b',                    # 10-17 digit account numbers
        ],
        'routing_number': [
            r'\b0\d{8}\b|\b1\d{8}\b|\b2\d{8}\b|\b3[0-2]\d{7}\b',  # Valid routing number ranges
        ],
        'credit_card': [
            r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',  # Credit card with separators
            r'\b\d{16}\b',                       # 16 digits continuous
        ],
        'phone': [
            r'\b\(\d{3}\)\s?\d{3}[-.\s]?\d{4}\b',  # (XXX) XXX-XXXX
            r'\b\d{3}[-.\s]\d{3}[-.\s]\d{4}\b',    # XXX-XXX-XXXX
        ],
        'email': [
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        ],
    }
    
    # What to replace with
    MASKS = {
        'ssn': '***-**-****',
        'ein': '**-*******',
        'bank_account': '[ACCOUNT-MASKED]',
        'routing_number': '[ROUTING-MASKED]',
        'credit_card': '****-****-****-****',
        'phone': '(***) ***-****',
        'email': '[EMAIL-MASKED]',
    }
    
    @classmethod
    def detect_pii(cls, text: str) -> Dict[str, List[str]]:
        """
        Detect PII in text and return found items by type.
        
        Args:
            text: Input text to scan
            
        Returns:
            Dictionary mapping PII type to list of found values
        """
        if not text:
            return {}
            
        found = {}
        for pii_type, patterns in cls.PATTERNS.items():
            matches = []
            for pattern in patterns:
                matches.extend(re.findall(pattern, text, re.IGNORECASE))
            if matches:
                found[pii_type] = list(set(matches))  # Remove duplicates
        return found
    
    @classmethod
    def mask_pii(cls, text: str) -> Tuple[str, Dict[str, int]]:
        """
        Mask all PII in text and return masked text + count of masked items.
        
        Args:
            text: Input text to mask
            
        Returns:
            Tuple of (masked_text, {pii_type: count})
        """
        if not text:
            return text, {}
            
        masked_text = text
        masked_counts = {}
        
        for pii_type, patterns in cls.PATTERNS.items():
            count = 0
            for pattern in patterns:
                matches = re.findall(pattern, masked_text, re.IGNORECASE)
                count += len(matches)
                masked_text = re.sub(pattern, cls.MASKS[pii_type], masked_text, flags=re.IGNORECASE)
            if count > 0:
                masked_counts[pii_type] = count
        
        return masked_text, masked_counts
    
    @classmethod
    def mask_ssn(cls, text: str) -> str:
        """Specifically mask only SSNs in text."""
        if not text:
            return text
        for pattern in cls.PATTERNS['ssn']:
            text = re.sub(pattern, cls.MASKS['ssn'], text)
        return text
    
    @classmethod
    def mask_ein(cls, text: str) -> str:
        """Specifically mask only EINs in text."""
        if not text:
            return text
        for pattern in cls.PATTERNS['ein']:
            text = re.sub(pattern, cls.MASKS['ein'], text)
        return text
    
    @classmethod
    def get_pii_warning(cls, detected: Dict[str, List[str]]) -> str:
        """
        Generate a user-friendly warning message based on detected PII.
        
        Args:
            detected: Dictionary of detected PII from detect_pii()
            
        Returns:
            Formatted warning string
        """
        if not detected:
            return ""
        
        warnings = ["âš ï¸ **Sensitive Information Detected & Masked:**"]
        
        pii_labels = {
            'ssn': 'Social Security Number(s)',
            'ein': 'Employer Identification Number(s)',
            'bank_account': 'Bank Account Number(s)',
            'routing_number': 'Routing Number(s)',
            'credit_card': 'Credit Card Number(s)',
            'phone': 'Phone Number(s)',
            'email': 'Email Address(es)',
        }
        
        for pii_type, items in detected.items():
            label = pii_labels.get(pii_type, pii_type)
            count = len(items) if isinstance(items, list) else items
            warnings.append(f"â€¢ {count} {label} detected and masked")
        
        warnings.append("\n*Your sensitive data has been automatically protected.*")
        
        return "\n".join(warnings)
    
    @classmethod
    def is_safe_to_process(cls, text: str) -> Tuple[bool, str]:
        """
        Check if text is safe to process (no critical PII).
        
        Args:
            text: Input text to check
            
        Returns:
            Tuple of (is_safe, warning_message)
        """
        detected = cls.detect_pii(text)
        
        # Critical PII types that should trigger warnings
        critical_types = ['ssn', 'bank_account', 'credit_card']
        
        critical_found = {k: v for k, v in detected.items() if k in critical_types}
        
        if critical_found:
            return False, cls.get_pii_warning(critical_found)
        
        return True, ""


# ==========================================
# Quick Test (remove in production)
# ==========================================
if __name__ == "__main__":
    # Test PII detection
    test_text = """
    My SSN is 123-45-6789 and my employer's EIN is 12-3456789.
    Call me at (555) 123-4567 or email test@example.com.
    My account number is 12345678901234.
    """
    
    print("Original text:")
    print(test_text)
    print("\n" + "="*50 + "\n")
    
    # Detect
    detected = PIIHandler.detect_pii(test_text)
    print("Detected PII:")
    for pii_type, values in detected.items():
        print(f"  {pii_type}: {values}")
    
    print("\n" + "="*50 + "\n")
    
    # Mask
    masked, counts = PIIHandler.mask_pii(test_text)
    print("Masked text:")
    print(masked)
    print(f"\nMasked counts: {counts}")
    
    print("\n" + "="*50 + "\n")
    
    # Warning
    print("Warning message:")
    print(PIIHandler.get_pii_warning(detected))

# Fix sqlite3 issue
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except (ImportError, KeyError):
    pass

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import Tool
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import json
import re

# ==========================================
# Configuration
# ==========================================
DB_DIRECTORY = "federal_tax_vector_db"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
COLLECTION_NAME = "federal_tax_documents"

# ==========================================
# Data Structures
# ==========================================
class UserProfile(BaseModel):
    """User's complete tax profile"""
    citizenship_status: Optional[str] = Field(default=None)
    student_status: Optional[str] = Field(default=None)
    employment_details: Optional[str] = Field(default=None)
    tax_filing_experience: Optional[str] = Field(default=None)
    residency_duration: Optional[str] = Field(default=None)
    income: Optional[int] = Field(default=None)
    residency_state: Optional[str] = Field(default=None)
    name: Optional[str] = Field(default=None)
    filing_status: Optional[str] = Field(default=None)
    w2_forms_count: Optional[int] = Field(default=None)

# ==========================================
# 1. Intake Agent (Fixed)
# ==========================================
class IntakeAgent:
    """Hybrid Intake Agent with auto-extraction and friendly dialogue"""
    
    QUESTIONNAIRE = [
        "What is your citizenship status? (US Citizen / Green Card Holder / International Student / Other)",
        "Are you a student? (Full-time / Part-time / Not a student)",
        "What is your employment status? (On-campus job / Off-campus job / Self-employed / Multiple jobs)",
        "Have you filed US taxes before? (Yes / No)",
        "How long have you lived in your current state?",
        "What was your total income last year? (Approximate)",
        "Which state do you currently live in?"
    ]
    
    CONVERSATIONAL_PROMPT = """I'm your AI tax assistant! ðŸ‘‹

I notice you need help with your taxes. To give you the best advice, I'd like to learn a bit about your situation.

You can either:
1. **Answer these quick questions:**
{questions}

2. **Or just tell me naturally**, like:
   - "I'm an international student on F-1 visa, working on-campus..."
   - "I'm a working professional in New York..."

(These are just examples! Please tell me YOUR situation. ðŸ˜Š)"""
    
    def __init__(self, llm):
        self.llm = llm
        
        # âœ… FIX: å®šä¹‰é˜²å¹»è§‰çš„æå–æŒ‡ä»¤
        extraction_system_prompt = """You are an expert data extraction agent.
        Your task is to extract user profile information into a structured JSON format.
        
        CRITICAL RULES:
        1. ONLY extract information that is EXPLICITLY stated in the user's input.
        2. If a field is not mentioned, leave it as null (None).
        3. DO NOT infer or guess information.
        4. DO NOT use example data (like "California" or "$60k") unless the user explicitly claims it.
        5. If the user only says greetings like "hi", "hello", or asks a question without providing personal info, return an EMPTY profile.
        """
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", extraction_system_prompt),
            ("human", "{input}"),
        ])
        
        # âœ… å°† Prompt å’Œ LLM ç»‘åœ¨ä¸€èµ·
        self.extractor = prompt | llm.with_structured_output(UserProfile)
    
    def get_questionnaire(self) -> str:
        questions = "\n".join([f"   â€¢ {q}" for q in self.QUESTIONNAIRE])
        return self.CONVERSATIONAL_PROMPT.format(questions=questions)
    
    def extract_info(self, user_input: str) -> UserProfile:
        try:
            # invoke çŽ°åœ¨æŽ¥å— dictionary å› ä¸ºæˆ‘ä»¬åŠ äº† prompt template
            return self.extractor.invoke({"input": user_input})
        except Exception as e:
            print(f"âš ï¸ Intake extraction failed: {e}")
            return UserProfile()
    
    def check_completeness(self, profile: UserProfile) -> Dict[str, Any]:
        required = ['citizenship_status', 'student_status', 'employment_details', 
                    'tax_filing_experience', 'income', 'residency_state']
        missing = [f for f in required if getattr(profile, f) is None]
        return {
            'complete': len(missing) == 0,
            'missing_fields': missing,
            'completion_rate': (len(required) - len(missing)) / len(required) * 100
        }
    
    def get_smart_followup(self, profile: UserProfile) -> str:
        completeness = self.check_completeness(profile)
        if completeness['complete']:
            return "âœ… Perfect! I have everything I need. What would you like help with today?"
        
        if completeness['completion_rate'] == 0:
            return self.get_questionnaire()
        
        friendly_questions = {
            'citizenship_status': "your citizenship status",
            'student_status': "if you're currently a student",
            'employment_details': "your employment situation",
            'income': "your approximate income last year",
            'residency_state': "which state you live in",
            'tax_filing_experience': "if you've filed US taxes before",
        }
        
        missing = completeness['missing_fields'][:3]
        missing_text = ", ".join([friendly_questions.get(f, f) for f in missing])
        
        return f"""Great! I've got some of your info ({completeness['completion_rate']:.0f}% complete).

To give you better guidance, I'd like to know {missing_text}.

**Or just ask your tax question directly!** ðŸš€"""

# ==========================================
# 2. RAG Agent - With Enhanced Visual Support
# ==========================================
class RAGAgent:
    """RAG Agent with ChromaDB retrieval and visual mapping support"""
    
    def __init__(self, llm):
        self.llm = llm
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        
        if os.path.exists(DB_DIRECTORY):
            self.db = Chroma(
                persist_directory=DB_DIRECTORY,
                embedding_function=self.embeddings,
                collection_name=COLLECTION_NAME
            )
            print("âœ… RAG Agent: Connected to ChromaDB")
        else:
            print("âš ï¸ RAG Agent: ChromaDB not found")
            self.db = None
        
        self._build_qa_chain()
    
    def _build_qa_chain(self):
        if not self.db:
            self.qa_chain = None
            return
        
        template = """You are a tax expert assistant. Answer based on IRS documentation.

User Profile:
- Citizenship: {citizenship_status}
- Student Status: {student_status}
- Employment: {employment_details}
- Income: ${income}
- State: {residency_state}

IRS Documentation:
{context}

User Question: {question}

Provide a clear, helpful answer tailored to this user's situation."""

        prompt = ChatPromptTemplate.from_template(template)
        
        def retrieve_and_format(inputs):
            query = inputs["question"]
            docs = self.db.similarity_search(query, k=3)
            if not docs:
                return "No relevant information found."
            
            formatted = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get('source_file', 'Unknown')
                form = doc.metadata.get('form_number', 'N/A')
                content = doc.page_content[:400]
                formatted.append(f"Source {i} - {source} (Form {form}):\n{content}...")
            return "\n\n".join(formatted)
        
        self.qa_chain = (
            {
                "context": retrieve_and_format,
                "question": lambda x: x["question"],
                "citizenship_status": lambda x: x.get("citizenship_status", "Unknown"),
                "student_status": lambda x: x.get("student_status", "Unknown"),
                "employment_details": lambda x: x.get("employment_details", "Unknown"),
                "income": lambda x: x.get("income", "Unknown"),
                "residency_state": lambda x: x.get("residency_state", "Unknown"),
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )
    
    def search(self, query: str, doc_type: str = "all", k: int = 3) -> str:
        """Basic search returning formatted results"""
        if not self.db:
            return "Tax database is not available."
        
        try:
            filter_dict = {"doc_type": doc_type} if doc_type != "all" else None
            results = self.db.similarity_search(query, k=k, filter=filter_dict)
            
            if not results:
                return "No relevant information found."
            
            response = "Information from IRS Documents:\n\n"
            for i, doc in enumerate(results, 1):
                source = doc.metadata.get('source_file', 'Unknown')
                form = doc.metadata.get('form_number', 'N/A')
                content = doc.page_content[:300]
                response += f"Source {i} - {source} (Form {form}):\n{content}...\n\n"
            return response
        except Exception as e:
            return f"Error searching database: {str(e)}"
    
    def search_form_mapping(self, source_form: str, target_form: str, field: str = None) -> str:
        """
        Search for specific form-to-form field mappings
        E.g., W-2 Box 1 -> Form 1040-NR Line 1a
        """
        if not self.db:
            return "Tax database is not available."
        
        # Build targeted query
        if field:
            query = f"{source_form} {field} to {target_form} mapping instructions"
        else:
            query = f"{source_form} to {target_form} field mapping instructions"
        
        try:
            # Search with form-specific filters
            results = self.db.similarity_search(query, k=5)
            
            if not results:
                return f"No mapping information found for {source_form} to {target_form}."
            
            # Format for visual generation
            mapping_info = []
            for doc in results:
                source = doc.metadata.get('source_file', 'Unknown')
                form_num = doc.metadata.get('form_number', 'N/A')
                content = doc.page_content[:500]
                mapping_info.append({
                    "source": source,
                    "form": form_num,
                    "content": content
                })
            
            return mapping_info
        except Exception as e:
            return f"Error searching mappings: {str(e)}"
    
    def answer_with_context(self, query: str, user_profile: UserProfile) -> str:
        if not self.qa_chain:
            prompt = f"""You are a tax expert assistant.
User Profile: {user_profile.dict(exclude_none=True)}
Question: {query}
Provide helpful tax guidance."""
            response = self.llm.invoke(prompt)
            return response.content
        
        try:
            chain_input = {
                "question": query,
                "citizenship_status": user_profile.citizenship_status or "Unknown",
                "student_status": user_profile.student_status or "Unknown",
                "employment_details": user_profile.employment_details or "Unknown",
                "income": user_profile.income or "Unknown",
                "residency_state": user_profile.residency_state or "Unknown",
            }
            return self.qa_chain.invoke(chain_input)
        except Exception as e:
            return f"Error generating answer: {str(e)}"

# ==========================================
# 3. Tool Agent
# ==========================================
class ToolAgent:
    """Tax calculation tools"""
    
    @staticmethod
    def calculate_tax(income: int, filing_status: str = "single") -> str:
        standard_deductions = {
            "single": 14600, "married_jointly": 29200,
            "married_separately": 14600, "head_of_household": 21900
        }
        
        tax_brackets = [
            (11600, 0.10), (47150, 0.12), (100525, 0.22),
            (191950, 0.24), (243725, 0.32), (609350, 0.35), (float('inf'), 0.37)
        ]
        
        status = filing_status.lower().replace(" ", "_")
        deduction = standard_deductions.get(status, 14600)
        taxable_income = max(0, income - deduction)
        
        tax = 0
        prev_bracket = 0
        for bracket, rate in tax_brackets:
            if taxable_income <= bracket:
                tax += (taxable_income - prev_bracket) * rate
                break
            else:
                tax += (bracket - prev_bracket) * rate
                prev_bracket = bracket
        
        effective_rate = round((tax / income * 100), 2) if income > 0 else 0
        
        return f"""Tax Calculation Results:
- Gross Income: ${income:,}
- Standard Deduction: ${deduction:,}
- Taxable Income: ${taxable_income:,}
- Estimated Tax: ${round(tax, 2):,}
- Effective Tax Rate: {effective_rate}%

This is an estimate based on 2024 federal tax rates."""

# ==========================================
# 4. Visual Agent - NEW: RAG-Enhanced Visuals
# ==========================================
class VisualAgent:
    """
    Generates step-by-step visual guides for form mappings
    Uses RAG to retrieve accurate IRS documentation
    """
    
    def __init__(self, llm, rag_agent: RAGAgent):
        self.llm = llm
        self.rag = rag_agent
        self.generated_snippets = {}  # {topic: [snippet1, snippet2, ...]}
    
    def infer_topic(self, messages: List[dict], user_profile: UserProfile) -> str:
        """Infer the most relevant visual topic from conversation"""
        recent_text = "\n".join([
            f"{m.get('role', 'user')}: {m.get('content', '')}" 
            for m in messages[-10:]
        ])
        
        profile_str = json.dumps(user_profile.dict(exclude_none=True), indent=2)
        
        prompt = f"""Based on this tax conversation, determine the most relevant form mapping topic.

Conversation:
{recent_text or "[no messages yet]"}

User Profile:
{profile_str}

Return EXACTLY ONE topic key from these options:
- w2_to_1040nr (W-2 to Form 1040-NR for nonresidents)
- w2_to_1040 (W-2 to Form 1040 for residents)
- 1098t_to_1040nr (Form 1098-T tuition to 1040-NR)
- 1098t_to_1040 (Form 1098-T to Form 1040)
- 1099int_to_1040 (1099-INT interest income)
- 1099nec_to_schedule_c (1099-NEC self-employment)
- schedule1_adjustments (Schedule 1 adjustments)
- generic_tax_visual (general guidance)

Rules:
- International students/F-1 visa â†’ use 1040nr variants
- US citizens/residents â†’ use 1040 variants
- Students with tuition â†’ 1098t topics
- Self-employed â†’ 1099nec or schedule_c
- Default: w2_to_1040nr for students, w2_to_1040 for professionals

Respond with ONLY the topic key, nothing else."""

        try:
            response = self.llm.invoke(prompt)
            topic = response.content.strip().lower().replace("-", "_")
            # Validate topic
            valid_topics = [
                "w2_to_1040nr", "w2_to_1040", "1098t_to_1040nr", "1098t_to_1040",
                "1099int_to_1040", "1099nec_to_schedule_c", "schedule1_adjustments",
                "generic_tax_visual"
            ]
            if topic not in valid_topics:
                topic = "w2_to_1040nr"
            return topic
        except Exception as e:
            print(f"âš ï¸ Topic inference failed: {e}")
            return "w2_to_1040nr"
    
    def _parse_topic(self, topic: str) -> Dict[str, str]:
        """Parse topic key into source and target forms"""
        mappings = {
            "w2_to_1040nr": {"source": "W-2", "target": "1040-NR"},
            "w2_to_1040": {"source": "W-2", "target": "1040"},
            "1098t_to_1040nr": {"source": "1098-T", "target": "1040-NR"},
            "1098t_to_1040": {"source": "1098-T", "target": "1040"},
            "1099int_to_1040": {"source": "1099-INT", "target": "1040"},
            "1099nec_to_schedule_c": {"source": "1099-NEC", "target": "Schedule C"},
            "schedule1_adjustments": {"source": "Various", "target": "Schedule 1"},
            "generic_tax_visual": {"source": "General", "target": "Tax Return"},
        }
        return mappings.get(topic, {"source": "W-2", "target": "1040-NR"})
    
    def generate_visual_snippet(self, topic: str, user_profile: UserProfile) -> str:
        """
        Generate the NEXT visual snippet for a topic using RAG
        """
        existing = self.generated_snippets.get(topic, [])
        step_number = len(existing) + 1
        
        # Parse topic to get source/target forms
        forms = self._parse_topic(topic)
        source_form = forms["source"]
        target_form = forms["target"]
        
        # ========== RAG INTEGRATION ==========
        # Query ChromaDB for relevant form mapping information
        rag_context = ""
        if self.rag and self.rag.db:
            # Build step-specific query
            step_queries = {
                1: f"{source_form} Box 1 wages {target_form}",
                2: f"{source_form} Box 2 federal tax withheld {target_form}",
                3: f"{source_form} Box 3 4 Social Security {target_form}",
                4: f"{source_form} Box 5 6 Medicare {target_form}",
                5: f"{source_form} Box 12 14 other information {target_form}",
            }
            query = step_queries.get(step_number, f"{source_form} to {target_form} mapping step {step_number}")
            
            try:
                docs = self.rag.db.similarity_search(query, k=2)
                if docs:
                    rag_context = "\n\n".join([
                        f"IRS Reference ({doc.metadata.get('source_file', 'Unknown')}):\n{doc.page_content[:300]}"
                        for doc in docs
                    ])
            except Exception as e:
                print(f"âš ï¸ RAG search failed: {e}")
        # =====================================
        
        profile_str = json.dumps(user_profile.dict(exclude_none=True), indent=2)
        
        system_prompt = """You are a tax visualization expert. Create step-by-step visual guides 
showing how to map values from source tax forms to destination forms.

Your output should be a code-style text block with:
- Clear header with step number and focus
- Box-to-line mappings using arrows (â†’)
- Specific box numbers and line numbers
- Brief explanations
- Example values where helpful"""

        user_prompt = f"""Create Step {step_number} of a visual guide for: {source_form} â†’ {target_form}

User Profile:
{profile_str}

{"IRS Documentation Reference:" + chr(10) + rag_context if rag_context else ""}

Previous steps completed: {step_number - 1}

Requirements:
1. Start with a header block like:
   ðŸ“‹ {source_form} â†’ {target_form} Mapping (Step {step_number}/5)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Focus: [specific focus for this step]

2. Show 1-2 specific box-to-line mappings with arrows
3. Include brief explanation of what each value represents
4. Add example if helpful
5. End with a separator line
6. Keep under 150 words

For step {step_number}, focus on:
- Step 1: Wages/compensation (Box 1)
- Step 2: Federal tax withheld (Box 2)  
- Step 3: Social Security (Boxes 3-4)
- Step 4: Medicare (Boxes 5-6)
- Step 5: Other codes and state info (Boxes 12, 14)"""

        try:
            response = self.llm.invoke(f"{system_prompt}\n\n{user_prompt}")
            snippet = response.content.strip()
            
            # Store the snippet
            if topic not in self.generated_snippets:
                self.generated_snippets[topic] = []
            self.generated_snippets[topic].append(snippet)
            
            return snippet
        except Exception as e:
            return f"Error generating visual: {str(e)}"
    
    def get_all_snippets(self, topic: str) -> List[str]:
        """Get all generated snippets for a topic"""
        return self.generated_snippets.get(topic, [])
    
    def reset_topic(self, topic: str = None):
        """Reset snippets for a topic or all topics"""
        if topic:
            self.generated_snippets[topic] = []
        else:
            self.generated_snippets = {}

# ==========================================
# 5. Checklist Agent
# ==========================================
class ChecklistAgent:
    """Generates and maintains tax filing checklist"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def generate_checklist(self, conversation_history: List[dict], user_profile: UserProfile) -> List[dict]:
        if not conversation_history:
            return []
        
        convo_text = "\n".join([
            f"{msg.get('role', 'user').upper()}: {msg.get('content', '')}" 
            for msg in conversation_history
        ])
        
        system_prompt = """You are a CHECKLIST AGENT for US tax filing.

CRITICAL RULES:
1. DO NOT extract user information from the conversation - only create task checklist
2. IGNORE any example data like "I'm an international student earning $15k" that appears in the assistant's suggestions
3. Only mark items as "done" based on ACTUAL user responses, NOT example text from the assistant
4. If the assistant is showing a questionnaire with examples, ALL items should be "pending"

Return ONLY valid JSON in this format:
{
  "sections": [
    {
      "heading": "Collect W-2 forms",
      "status": "pending",
      "details": [
        {"item": "Collect W-2 from each employer", "status": "pending"},
        {"item": "Record wages (Box 1)", "status": "pending"}
      ]
    }
  ]
}

Rules:
- ACTION headings (e.g., "Collect W-2 forms", "Complete Form 1040-NR")
- 3-7 detailed sub-items per section
- Mark "done" ONLY if user's OWN messages (not assistant examples) mention completing it
- Tailor to actual user profile data passed separately, NOT from conversation
- 4-8 sections total
- Return ONLY JSON, no explanations"""

        # âœ… æ˜Žç¡®ä¼ é€’çœŸå®žçš„ user profileï¼ˆä¸æ˜¯ä»Žå¯¹è¯æå–ï¼‰
        user_prompt = f"""ACTUAL User Profile (use this, not conversation examples):
{json.dumps(user_profile.dict(exclude_none=True), indent=2)}

Conversation (IGNORE example data in assistant messages):
{convo_text}

Generate the checklist based on the ACTUAL user profile above:"""

        try:
            response = self.llm.invoke(f"{system_prompt}\n\n{user_prompt}")
            content = response.content if hasattr(response, 'content') else str(response)
            
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                sections = data.get("sections", [])
                
                for section in sections:
                    details = section.get("details", [])
                    if details:
                        done_count = sum(1 for d in details if d.get("status") == "done")
                        section["completion"] = int((done_count / len(details)) * 100)
                    else:
                        section["completion"] = 0
                    
                    section["status"] = "done" if section["completion"] == 100 else "pending"
                
                return sections
        except Exception as e:
            print(f"âš ï¸ Checklist generation failed: {e}")
        
        return []

# ==========================================
# 6. Orchestrator Agent
# ==========================================
class OrchestratorAgent:
    """Central coordinator using LLM-enhanced decision making"""
    
    def __init__(self, llm, intake, rag, tool, visual):
        self.llm = llm
        self.intake = intake
        self.rag = rag
        self.tool = tool
        self.visual = visual
    
    def route(self, user_input: str, user_profile: UserProfile) -> str:
        user_lower = user_input.lower().strip()
        
        # Simple greetings
        if user_lower in ['hi', 'hello', 'hey', 'start', 'begin', 'help']:
            completeness = self.intake.check_completeness(user_profile)
            if not completeness['complete']:
                return self.intake.get_questionnaire()
            return self.intake.get_smart_followup(user_profile)
        
        # LLM decision for complex queries
        return self._llm_decide_and_act(user_input, user_profile)
    
    def _llm_decide_and_act(self, user_input: str, user_profile: UserProfile) -> str:
        decision_prompt = f"""Analyze what the user needs.

User Profile: {user_profile.dict(exclude_none=True)}
Question: {user_input}

Decide: SEARCH, CALCULATE, BOTH, or DIRECT
Respond with ONE WORD only."""

        try:
            decision = self.llm.invoke(decision_prompt)
            action = decision.content.strip().upper()
            print(f"ðŸ¤– LLM Decision: {action}")
            
            if action == "CALCULATE" and user_profile.income:
                return self.tool.calculate_tax(user_profile.income)
            elif action == "SEARCH":
                context = self.rag.search(user_input)
                return self._synthesize(user_input, user_profile, context)
            elif action == "BOTH":
                context = self.rag.search(user_input)
                tax_info = self.tool.calculate_tax(user_profile.income) if user_profile.income else ""
                return self._synthesize(user_input, user_profile, context, tax_info)
            else:
                return self.rag.answer_with_context(user_input, user_profile)
        except Exception as e:
            return self.rag.answer_with_context(user_input, user_profile)
    
    def _synthesize(self, question: str, profile: UserProfile, context: str, tax_info: str = "") -> str:
        prompt = f"""Provide a helpful answer combining this information:

User Profile: {profile.dict(exclude_none=True)}
Question: {question}
IRS Documentation: {context}
{"Tax Calculation: " + tax_info if tax_info else ""}

Be friendly and clear! ðŸ˜Š"""
        
        response = self.llm.invoke(prompt)
        return response.content

# ==========================================
# 7. Main Orchestrator (External Interface)
# ==========================================
class TaxOrchestrator:
    """Main entry point managing all agents"""
    
    def __init__(self, api_key: str):
        if not api_key:
            raise ValueError("GOOGLE_API_KEY is required")
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=api_key,
            temperature=0
        )
        
        print("ðŸš€ Initializing Tax Assistant System...")
        
        self.intake_agent = IntakeAgent(self.llm)
        print("âœ… Intake Agent ready")
        
        self.rag_agent = RAGAgent(self.llm)
        print("âœ… RAG Agent ready")
        
        self.tool_agent = ToolAgent()
        print("âœ… Tool Agent ready")
        
        # NEW: Visual Agent with RAG integration
        self.visual_agent = VisualAgent(self.llm, self.rag_agent)
        print("âœ… Visual Agent ready (RAG-enhanced)")
        
        self.checklist_agent = ChecklistAgent(self.llm)
        print("âœ… Checklist Agent ready")
        
        self.orchestrator = OrchestratorAgent(
            self.llm, self.intake_agent, self.rag_agent, 
            self.tool_agent, self.visual_agent
        )
        print("âœ… Orchestrator ready")
        print("=" * 50)
    
    def run_orchestrator(self, user_input: str, user_profile: UserProfile = None) -> dict:
        if user_profile is None:
            user_profile = UserProfile()
        response = self.orchestrator.route(user_input, user_profile)
        return {"output": response}
    
    def run_intake(self, user_input: str) -> UserProfile:
        return self.intake_agent.extract_info(user_input)
    
    def generate_checklist(self, conversation_history: List[dict], user_profile: UserProfile = None) -> List[dict]:
        if user_profile is None:
            user_profile = UserProfile()
        return self.checklist_agent.generate_checklist(conversation_history, user_profile)
    
    # NEW: Visual generation methods
    def infer_visual_topic(self, messages: List[dict], user_profile: UserProfile) -> str:
        return self.visual_agent.infer_topic(messages, user_profile)
    
    def generate_visual_step(self, topic: str, user_profile: UserProfile) -> str:
        return self.visual_agent.generate_visual_snippet(topic, user_profile)
    
    def get_visual_snippets(self, topic: str) -> List[str]:
        return self.visual_agent.get_all_snippets(topic)
    
    def reset_visuals(self, topic: str = None):
        self.visual_agent.reset_topic(topic)